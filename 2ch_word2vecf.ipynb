{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ugly_normalize(vecs):\n",
    "    normalizers = np.sqrt((vecs * vecs).sum(axis=1))\n",
    "    normalizers[normalizers == 0] = 1\n",
    "    return (vecs.T / normalizers).T\n",
    "\n",
    "\n",
    "class Embeddings:\n",
    "    def __init__(self, vecsfile, vocabfile=None, normalize=True):\n",
    "        if vocabfile is None: vocabfile = vecsfile.replace(\"npy\", \"vocab\")\n",
    "        self._vecs = np.load(vecsfile)\n",
    "        self._vocab = open(vocabfile).read().split()\n",
    "        if normalize:\n",
    "            self._vecs = ugly_normalize(self._vecs)\n",
    "        self._w2v = {w: i for i, w in enumerate(self._vocab)}\n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, vecsfile, vocabfile=None):\n",
    "        return Embeddings(vecsfile, vocabfile)\n",
    "\n",
    "    \n",
    "    def word2vec(self, w):\n",
    "        return self._vecs[self._w2v[w]]\n",
    "\n",
    "    \n",
    "    def similar_to_vec(self, v, N=10):\n",
    "        sims = self._vecs.dot(v)\n",
    "        sims = heapq.nlargest(N, zip(sims, self._vocab, self._vecs))\n",
    "        return sims\n",
    "\n",
    "    \n",
    "    def most_similar(self, word, N=10):\n",
    "        w = self._vocab.index(word)\n",
    "        sims = self._vecs.dot(self._vecs[w])\n",
    "        sims = heapq.nlargest(N, zip(sims, self._vocab))\n",
    "        return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(word1, word2):\n",
    "    return 1 - spatial.distance.cosine(wv(word1), wv(word2))\n",
    "\n",
    "def wv(word):\n",
    "    return glove.word_vectors[glove.dictionary[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2vf_path = '/home/defeater/Documents/NLP/word2vecf'\n",
    "w2vf = Embeddings.load(path.join(w2vf_path, 'vecs.npy'), path.join(w2vf_path, 'vecs.vocab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_vocab = w2vf._vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['можно',\n",
       " 'есть',\n",
       " 'что',\n",
       " 'так',\n",
       " 'и',\n",
       " 'нужно',\n",
       " 'у',\n",
       " 'это',\n",
       " 'не',\n",
       " 'меня',\n",
       " 'как',\n",
       " 'нет',\n",
       " 'тебя',\n",
       " 'может',\n",
       " 'же',\n",
       " 'то',\n",
       " 'надо',\n",
       " 'сделать']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
